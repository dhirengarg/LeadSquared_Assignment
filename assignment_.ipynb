{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e06e7b34",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Problem statement \n",
    "\n",
    "    Objective: \n",
    "    To get the top relevent customer reviews about the product so that company could get the insights about their products. \n",
    "    \n",
    "    Assumptions:\n",
    "    1. If I could show the top_n and botton_n reviews about the product based on the polarity(sentiment of review) and ratings, this could give insights to customers about their product best and worst features. \n",
    "    \n",
    "    About Notebook:\n",
    "    I started with EDA to get familier with data, then doing some data cleaning techniques followed with creating features using tfidf and countvectorizer. Used few Classification models like . Naivebayes, Randomforest and XGboost. \n",
    "    Choose Randomforest and predict the most relevent reviews. \n",
    "    \n",
    "    Then I tried LSTM to further improve the F-score. \n",
    "    \n",
    "    \n",
    "    Future Enhancements:\n",
    "    1. BERT base models could be implemented \n",
    "    2. More embedding techniques could be explored to make features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ad0a8",
   "metadata": {},
   "source": [
    "#### Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c55a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import contractions\n",
    "import re \n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from textblob import TextBlob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt \n",
    "import string\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import sklearn \n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import auc, plot_precision_recall_curve\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6004e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50efb0c4",
   "metadata": {},
   "source": [
    "#### Getting datafram from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73a94fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\LeadSQ\\Womens Clothing E-Commerce Reviews.csv', index_col= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d259d5",
   "metadata": {},
   "source": [
    "#### Basic EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68a9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2617d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3298573",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15642b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5751734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c63528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b7a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clothing ID'].value_counts() \n",
    "#1206 unique IDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50ecc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Clothing ID'] ==776].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac7059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Rating'].value_counts()\n",
    "# data is skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50adfedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Recommended IND'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5907df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Positive Feedback Count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d7d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Positive Feedback Count'] == 59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b767a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(data=df, x=\"Positive Feedback Count\")\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf4c8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4f670f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939adba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(data=df, x=\"Rating\")\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "    \n",
    "# Data is highly skewed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb1d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(data=df, hue=\"Recommended IND\",  x=\"Rating\")\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "    \n",
    "# we could see that even for rating = 5, there are 25 entries which has recommended ind = 0 \n",
    "# we could see that even for rating = 1, there are 16 entries which has recommended ind = 1\n",
    "# these all the outliers.. so\n",
    "# for rating = 3, these are the ambiguous ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3438c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Rating'] == 5) & (df['Recommended IND'] ==0 )].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc2b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Rating'] == 1) & (df['Recommended IND'] ==1)].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d480722",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Rating'] == 5) & (df['Recommended IND'] ==0 )].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89fea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop(df[(df['Rating'] == 1) & (df['Recommended IND'] ==1)].index)\n",
    "df= df.drop(df[(df['Rating'] == 2) & (df['Recommended IND'] ==1)].index)\n",
    "df= df.drop(df[(df['Rating'] == 5) & (df['Recommended IND'] ==0 )].index)\n",
    "df= df.drop(df[(df['Rating'] == 4) & (df['Recommended IND'] ==0 )].index)\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590ba3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903cba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Division Name'].value_counts(dropna = False)\n",
    "\n",
    "#manual rating (i will go data labelling team)\n",
    "#remove the outliers \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bb0703",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(data=df, x=\"Division Name\", hue=\"Recommended IND\")\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0177a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Department Name'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03e97ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(data=df, x=\"Department Name\", hue=\"Recommended IND\")\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a8d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class Name'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969e5168",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Class Name','Clothing ID']].groupby(['Class Name']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b9bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check combined NULLs in Review text column and in Title columns.. These cud be combined to get full text. \n",
    "# doing this way wud reduce the NULL records. \n",
    "df_nulls = df[((df['Review Text'].isnull()) & df['Title'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34bfe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nulls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab4c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(data=df_nulls,  x=\"Rating\")\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "# total 844 such records - having null review_Text and title. \n",
    "# 590 such records which has null review text and Title... and still rating is 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60e1b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nulls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d473b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review Text']=df['Review Text'].apply(str)\n",
    "df['Title']=df['Title'].apply(str)\n",
    "\n",
    "# combining the Review Text and Title columns to get single columns .. Review \n",
    "df['Review'] = df['Title'] + ' ' + df['Review Text']\n",
    "df['Review']=df['Review'].apply(str)\n",
    "df['Review_len'] = df['Review'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef8985",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Review_len']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf4fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.iloc[:22000]\n",
    "df_test = df.iloc[22000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7cd37a",
   "metadata": {},
   "source": [
    "#### Data Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d084a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_train[['Review', 'Recommended IND', 'Rating' ,'Review_len', 'Class Name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe661179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated_characters(tokens):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    old_word = tokens\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')   #check this expression. \n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "\n",
    "\n",
    "def process_text(document):\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)  #to remove multiple white space character \n",
    "        document = re.sub(r'\\W', ' ', str(document))\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "        document = re.sub(r'[^a-zA-Z]', ' ', document)\n",
    "        document = document.lower() \n",
    "        tokens = document.split()\n",
    "        tokens = [contractions.fix(word) for word in tokens]\n",
    "        tokens = remove_repeated_characters(tokens)\n",
    "        #lemma_txt = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        #lemma_no_stop_txt = [word for word in lemma_txt if word not in stopwords]\n",
    "        clean_txt = ' '.join(tokens)\n",
    "        return clean_txt\n",
    "    \n",
    "\n",
    "def wordcloud_text(data):\n",
    "    text = \" \".join(review for review in data)\n",
    "    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def get_pos(review):\n",
    "    lis = []\n",
    "    doc = nlp(review)\n",
    "    for token in doc: \n",
    "        if (token.pos_ != 'PRON' and token.pos_ != 'DET' and token.pos_ != 'SCONJ' and token.pos_ != 'ADP'\n",
    "        and token.pos_ != 'PROPN' and token.pos_ != 'CCONJ' and token.pos_ != 'AUX'):\n",
    "            lis.append(token.text)\n",
    "    return ' '.join(lis)\n",
    "\n",
    "\n",
    "def get_most_common (reviews, top_n ):\n",
    "    lis = []\n",
    "    for reviews in reviews:\n",
    "        for words in reviews.split():\n",
    "            lis.append(words)\n",
    "    my_counter = Counter(lis)\n",
    "    top_n = my_counter.most_common(top_n)\n",
    "    return(top_n)\n",
    "\n",
    "def extract_ngrams(reviews, num):\n",
    "    for review in reviews:\n",
    "        n_grams = ngrams(nltk.word_tokenize(review), num)\n",
    "        sent =  [ '_'.join(grams) for grams in n_grams]\n",
    "    return (' '.join(sent))\n",
    "\n",
    "def unique_words(data):\n",
    "    lis = []\n",
    "    for review in data:\n",
    "        for word in review.split():\n",
    "            lis.append(word)\n",
    "    return (set(lis))\n",
    "\n",
    "\n",
    "\n",
    "def model_train(model, X_train, y_train,X_test,y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred=model.predict(X_test)\n",
    "    print(f\"accuracy score for {model} is {accuracy_score(y_pred, y_test)}\")\n",
    "    probs = model.predict_proba(X_test)\n",
    "    probs = probs[:, 1]\n",
    "    average_precision = average_precision_score(y_test, y_pred)\n",
    "    print(f\" average precision is {average_precision}\")\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "    auc_precision_recall = auc(recall, precision)\n",
    "    print(f\"auc precison recall is {auc_precision_recall}\")\n",
    "    plt.plot(recall, precision)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return classification_report(y_test, y_pred)\n",
    "\n",
    "\n",
    "def remov_letter (review, n_char):\n",
    "    lis = []\n",
    "    for word in review.split(): \n",
    "        if (len(word)>n_char):\n",
    "            lis.append(word)\n",
    "    return ' '.join(lis)\n",
    "\n",
    "\n",
    "def lemmatiz(document):\n",
    "        tokens = document.split()\n",
    "        lemma_txt = [lemmatizer.lemmatize(word) for word in tokens]        \n",
    "        clean_txt = ' '.join(lemma_txt)\n",
    "        return clean_txt\n",
    "    \n",
    "    \n",
    "def get_rating (row):\n",
    "    if (row['prdictions'] == 1) & (row['polarity']> df_polarity[df_polarity['Rating'].isin([4,5])]['polarity_mean'].mean()):\n",
    "        return 5 \n",
    "    if (row['prdictions'] == 1) & (row['polarity']<= df_polarity[df_polarity['Rating'].isin([4,5])]['polarity_mean'].mean()):\n",
    "        return 4\n",
    "    if (row['prdictions'] == 0) & (row['polarity']>= df_polarity[df_polarity['Rating'].isin([2,3])]['polarity_mean'].mean()):\n",
    "        return 3\n",
    "    if (row['prdictions'] == 0) & (row['polarity']>= df_polarity[df_polarity['Rating'].isin([1,2])]['polarity_mean'].mean()):\n",
    "        return 2\n",
    "    if (row['prdictions'] == 0) & (row['polarity']<= df_polarity[df_polarity['Rating'].isin([2])]['polarity_mean'].mean()):\n",
    "        return 1\n",
    "    \n",
    "\n",
    "df_polarity=df_new.groupby('Rating').agg({'polarity':'mean'}).rename(columns={'polarity':'polarity_mean'}).reset_index()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feeebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['processed_review'] = df_new['Review'].progress_apply(process_text)\n",
    "df_new['processed_review_len'] = df_new['processed_review'].str.len()\n",
    "df_new[['processed_review_len', 'Review_len']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e3215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = ' nan was really hoping to like this but it did not look the way it does on the model at least not on me the sharkbite hem is much more pronounced and looser the one in the photo looks like it was pinned back am and usually wear medium or large got medium and there was lot more material on the bottom half than the photo shows it made me look bigger and was not flattering material has nice weave but it thin and delicate bought the holly deep olive and the blue colors'\n",
    "doc = nlp(review)\n",
    "for token in doc: \n",
    "    if (token.pos_ != 'PRON' and token.pos_ != 'DET' and token.pos_ != 'SCONJ' and token.pos_ != 'ADP'\n",
    "        and token.pos_ != 'PROPN' and token.pos_ != 'CCONJ' and token.pos_ != 'AUX' and token.pos_ != 'NOUN'):\n",
    "        print(token.pos_, token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e9a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_text(df_new[df_new.Rating <3].processed_review)\n",
    "# to know about the high level what are the most occuring words for rating <3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_text(df_new[df_new.Rating >3].processed_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bd9117",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['only_adj_Adv'] = df_new.processed_review.progress_apply(lambda x :get_pos(x))\n",
    "df_new['only_adj_Adv_len'] = df_new['only_adj_Adv'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff34725",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_text(df_new[df_new.Rating >3].only_adj_Adv)\n",
    "wordcloud_text(df_new[df_new.Rating <3].only_adj_Adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cecead7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new['polarity'] = df_new['only_adj_Adv'].progress_apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6857ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['subjectivity'] = df_new['only_adj_Adv'].progress_apply(lambda x: TextBlob(x).sentiment[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f4a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grp1 = df_new.groupby('Rating').agg({'subjectivity':'mean', 'polarity':'mean'}).rename(columns={'polarity':'polarity_mean', 'subjectivity':'subj_mean'}).reset_index()\n",
    "df_grp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07aa5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grp = df_new.groupby('Recommended IND').agg({'polarity':'mean', 'only_adj_Adv_len':'mean'}).rename(columns={'polarity':'polarity_mean', 'only_adj_Adv_len':'sent_len_mean'}).reset_index()\n",
    "df_grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a431652",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_word1 = get_most_common(df_new[df_new.Rating > 3].only_adj_Adv, 200)\n",
    "freq_word2 = get_most_common(df_new[df_new.Rating < 3].only_adj_Adv, 200)\n",
    "print(freq_word1)\n",
    "print(freq_word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5240b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unique_words(df_new[df_new.Rating < 3].only_adj_Adv)))\n",
    "print(len(unique_words(df_new[df_new.Rating == 3].only_adj_Adv)))\n",
    "print(len(unique_words(df_new[df_new.Rating > 3].only_adj_Adv)))\n",
    "len(unique_words(df_new.only_adj_Adv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91b8e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_new[df_new['only_adj_Adv_len']==0].shape)\n",
    "df_new = df_new.drop(df_new[df_new['only_adj_Adv_len']==0].index)\n",
    "df_new.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13536f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[['processed_review_len', 'Review_len', 'only_adj_Adv_len']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e6292c",
   "metadata": {},
   "source": [
    "#### using countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(df_new['only_adj_Adv'], df_new['Recommended IND'],stratify= df_new['Recommended IND'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f64177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(min_df = .0001, max_df = .99, ngram_range = (1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e237a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_xtrain = cv.fit_transform(Xtrain)\n",
    "cv_xtest = cv.transform(Xtest)\n",
    "#cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdce25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix = cv_xtrain.toarray()\n",
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a72ad7c",
   "metadata": {},
   "source": [
    "#### using tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe7e9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(min_df = .0002, max_df = .99, ngram_range = (1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859e58ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_Xtrain = tfidf.fit_transform(Xtrain)\n",
    "tfidf_Xtest = tfidf.transform(Xtest)\n",
    "#cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_Xtrain.toarray()\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa81ef7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ba3d0d1",
   "metadata": {},
   "source": [
    "#### Selecting model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f50eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_train(MultinomialNB(), tfidf_Xtrain, Ytrain, tfidf_Xtest ,Ytest )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126d32df",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_train(MultinomialNB(), cv_xtrain, Ytrain, cv_xtest ,Ytest )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf05dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_train(RandomForestClassifier(n_estimators=1000, max_depth=5, \n",
    "        class_weight='balanced', random_state=3), cv_xtrain, Ytrain, cv_xtest ,Ytest)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd5ffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_train(RandomForestClassifier(n_estimators=1000, max_depth=5, \n",
    "        class_weight='balanced', random_state=3), tfidf_Xtrain, Ytrain, tfidf_Xtest ,Ytest)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb7a1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_train(XGBClassifier(), cv_xtrain, Ytrain, cv_xtest ,Ytest)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8d3541",
   "metadata": {},
   "source": [
    "#### Creating model, Dumping and Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c562a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tfidf, open(\"tfidfvectorizer.pickle\", \"wb\"))\n",
    "Rfmodel = RandomForestClassifier(n_estimators=1000, max_depth=5, class_weight='balanced', random_state=3)\n",
    "Rfmodel.fit(tfidf_Xtrain, Ytrain)\n",
    "pickle.dump(Rfmodel, open(\"rf.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c867b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = pickle.load(open(\"tfidfvectorizer.pickle\", \"rb\"))\n",
    "rf = pickle.load(open(\"rf.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867f048f",
   "metadata": {},
   "source": [
    "#### Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dad533",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Clothing ID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc5ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee3c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test['Clothing ID'] == 1094]['Class Name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77687ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ddf66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(clothids,className,Topn):\n",
    "    df1 = pd.DataFrame()\n",
    "    df_pred = pd.DataFrame()\n",
    "    df_pred = df_test[(df_test['Clothing ID'].isin(clothids)) & (df_test['Class Name'].isin(className))]\n",
    "    if (df_pred.shape[0] >0):\n",
    "        print(df_pred.shape[0])\n",
    "        df_pred['Review'] = df_pred['Title'] + ' ' + df_pred['Review Text']\n",
    "        df_pred['Review']=df_pred['Review'].apply(str)\n",
    "        df_pred['processed_review'] = df_pred['Review'].progress_apply(process_text)\n",
    "        df_pred['only_adj_Adv'] = df_pred.processed_review.progress_apply(lambda x :get_pos(x))\n",
    "        df_pred['polarity'] = df_pred['only_adj_Adv'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "        df_pred['only_adj_Adv_len'] = df_pred['only_adj_Adv'].str.len()\n",
    "        df_pred = df_pred.drop(df_pred[df_pred['only_adj_Adv_len']==0].index)\n",
    "        df_pred.reset_index(inplace=True)\n",
    "        print(df_pred.shape)\n",
    "        vectorizer = pickle.load(open(\"tfidfvectorizer.pickle\", \"rb\"))\n",
    "        tfidf_pred = vectorizer.transform(df_pred['only_adj_Adv'])\n",
    "        df_pred['prdictions'] = rf.predict(tfidf_pred).tolist()\n",
    "        df_pred['Predicted_Ratings'] = df_pred.progress_apply(lambda row:get_rating(row), axis=1)\n",
    "        df_fetch = df_pred[['Clothing ID', 'prdictions', 'Predicted_Ratings', 'polarity', 'Review Text', 'Title', 'Class Name']]\n",
    "        for id in clothids:\n",
    "            print(id)\n",
    "            for clas in className:\n",
    "                print(clas)\n",
    "                df_pos= df_fetch[(df_fetch['Clothing ID'] == id) & (df_fetch['Class Name'] == clas)].sort_values(by=['Predicted_Ratings','polarity'], ascending=False)\n",
    "                df1 = df1.append(df_pos[:Topn], ignore_index = True)\n",
    "                df_pos.drop(df_pos.index, inplace=True)\n",
    "                df_neg= df_fetch[(df_fetch['Clothing ID'] == id) & (df_fetch['Class Name'] == clas)].sort_values(by=[ 'Predicted_Ratings','polarity'], ascending=True)\n",
    "                df1 = df1.append(df_neg[:Topn], ignore_index = True)\n",
    "                df_neg.drop(df_neg.index, inplace=True)\n",
    "    return df1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40730ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict([1094, 1087],['Knits', 'Dresses'],3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15451418",
   "metadata": {},
   "source": [
    "#### Using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5a496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review Text']=df['Review Text'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2136e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_lstm, Xtest_lstm, Ytrain_lstm, Ytest_lstm = train_test_split(df['Review Text'], df['Recommended IND'],stratify= df['Recommended IND'], \n",
    "                                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfee157",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token='UNK')\n",
    "tokenizer.fit_on_texts(Xtrain_lstm)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "print('Vocabulary size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122efc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences = tokenizer.texts_to_sequences(Xtrain_lstm)\n",
    "X_train_padded_seq = pad_sequences(X_train_sequences, maxlen=300, padding='post', truncating='post')\n",
    "print(X_train_padded_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebeea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 13161 \n",
    "maxlen =  300 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f760d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "x = layers.Embedding(max_features, 128)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64,dropout=0.5))(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c245301",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train_padded_seq, Ytrain_lstm, batch_size=32, epochs=1, verbose=1)\n",
    "model.save('lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabd161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokenizer, open(\"lstmtokenizer.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64412807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84907add",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = load_model('lstm_model.h5')\n",
    "lstmvectorizer = pickle.load(open(\"lstmtokenizer.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1ff845",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sequences = lstmvectorizer.texts_to_sequences(Xtest_lstm)\n",
    "X_test_padded_seq = pad_sequences(X_test_sequences, maxlen=300, padding='post', truncating='post')\n",
    "predictions = model1.predict(X_test_padded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e18cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pd.DataFrame(predictions)  \n",
    "test_pred = test_pred.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf976bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat([Xtest_lstm,Ytest_lstm], axis=1 )  \n",
    "test_df = test_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da79e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d32ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292eaf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun1(x):\n",
    "    if x >=0.5:\n",
    "        x= 1 \n",
    "    else:\n",
    "        x = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal_Stack = pd.merge(test_pred, test_df, left_index=True, right_index=True)\n",
    "horizontal_Stack.drop(['index_x','index_y' ],  axis=1, inplace=True)\n",
    "horizontal_Stack['y_pred'] = horizontal_Stack[0].map(lambda x:fun1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0564d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal_Stack .shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89210b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(horizontal_Stack['Recommended IND'], horizontal_Stack['y_pred']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
